"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[7848],{4117:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"guideSidebar":[{"type":"link","label":"llm-d User Guide","href":"/llm-d.github.io/docs/guide/","docId":"guide/guide","unlisted":false},{"type":"category","label":"Installation","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Prerequisites","href":"/llm-d.github.io/docs/guide/Installation/prerequisites","docId":"guide/Installation/prerequisites","unlisted":false},{"type":"link","label":"Quick Start installer","href":"/llm-d.github.io/docs/guide/Installation/quickstart","docId":"guide/Installation/quickstart","unlisted":false}]}],"structureSidebar":[{"type":"link","label":"Overview of llm-d architecture","href":"/llm-d.github.io/docs/architecture/architecture","docId":"architecture/architecture","unlisted":false},{"type":"category","label":"Components","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Deployer","href":"/llm-d.github.io/docs/architecture/Components/deployer","docId":"architecture/Components/deployer","unlisted":false},{"type":"link","label":"Inference Simulator","href":"/llm-d.github.io/docs/architecture/Components/inf-simulator","docId":"architecture/Components/inf-simulator","unlisted":false},{"type":"link","label":"Inference Scheduler","href":"/llm-d.github.io/docs/architecture/Components/inf-scheduler","docId":"architecture/Components/inf-scheduler","unlisted":false},{"type":"link","label":"Disagg Prefill/Decode","href":"/llm-d.github.io/docs/architecture/Components/disagg_prefill-decode","className":"hidden","docId":"architecture/Components/disagg_prefill-decode","unlisted":false},{"type":"link","label":"Routing Sidecar","href":"/llm-d.github.io/docs/architecture/Components/routing-sidecar","docId":"architecture/Components/routing-sidecar","unlisted":false},{"type":"link","label":"KV-Cache Manager","href":"/llm-d.github.io/docs/architecture/Components/kv-cache","docId":"architecture/Components/kv-cache","unlisted":false}]},{"type":"category","label":"Media","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"llm-d Videos","href":"/llm-d.github.io/docs/architecture/Media/Videos","docId":"architecture/Media/Videos","unlisted":false}]}],"commSidebar":[{"type":"link","label":"Contribute to llm-d","href":"/llm-d.github.io/docs/community/contribute","docId":"community/contribute","unlisted":false},{"type":"link","label":"Talk to us!","href":"/llm-d.github.io/docs/community/contact_us","docId":"community/contact_us","unlisted":false}]},"docs":{"architecture/architecture":{"id":"architecture/architecture","title":"Overview of llm-d architecture","description":"llm-d is a Kubernetes-native distributed inference serving stack - a well-lit path for anyone to serve large language models at scale, with the fastest time-to-value and competitive performance per dollar for most models across most hardware accelerators.","sidebar":"structureSidebar"},"architecture/Components/deployer":{"id":"architecture/Components/deployer","title":"Deployer","description":"A key component in llm-d\'s toolbox is the llm-d deployer, the Helm chart for deploying llm-d on Kubernetes.","sidebar":"structureSidebar"},"architecture/Components/disagg_prefill-decode":{"id":"architecture/Components/disagg_prefill-decode","title":"Disaggregated Prefill/Decode Inference Serving in llm-d","description":"Overview","sidebar":"structureSidebar"},"architecture/Components/inf-scheduler":{"id":"architecture/Components/inf-scheduler","title":"llm-d Inference Router Architecture","description":"Overview","sidebar":"structureSidebar"},"architecture/Components/inf-simulator":{"id":"architecture/Components/inf-simulator","title":"vLLM Simulator","description":"To help with development and testing we have developed a light weight vLLM simulator. It does not truly","sidebar":"structureSidebar"},"architecture/Components/kv-cache":{"id":"architecture/Components/kv-cache","title":"KV-Cache Manager","description":"Introduction","sidebar":"structureSidebar"},"architecture/Components/routing-sidecar":{"id":"architecture/Components/routing-sidecar","title":"P/D Routing Sidecar","description":"This project provides a reverse proxy redirecting incoming requests","sidebar":"structureSidebar"},"architecture/Media/Videos":{"id":"architecture/Media/Videos","title":"Videos featuring llm-d","description":"Demo: llm-d - Distributed Inference for AI Runtimes","sidebar":"structureSidebar"},"community/contact_us":{"id":"community/contact_us","title":"Talk to us!","description":"There are several ways you can join the community effort to develop and enhance llm-d","sidebar":"commSidebar"},"community/contribute":{"id":"community/contribute","title":"Contribute to llm-d","description":"NOTE//github.com/llm-d/llm-d/blob/dev/PROJECT.md","sidebar":"commSidebar"},"contributing-to-docs":{"id":"contributing-to-docs","title":"Contributing to the Docs","description":"Contributing to the docs is very simple! All of the Docs section is created using MarkDown."},"guide/guide":{"id":"guide/guide","title":"llm-d User Guide","description":"The user guide is organized in sections to help you get started with llm-d and then tailor the configuration to your resources and application needs. It is currently focused on the Quick Start via the llmd-deployer Helm chart.","sidebar":"guideSidebar"},"guide/Installation/prerequisites":{"id":"guide/Installation/prerequisites","title":"Prerequisites for running the llm-d QuickStart","description":"Client Configuration","sidebar":"guideSidebar"},"guide/Installation/quickstart":{"id":"guide/Installation/quickstart","title":"Trying llm-d via the Quick Start installer","description":"Getting Started with llm-d on Kubernetes.  For specific instructions on how to install llm-d on minikube, see the README-minikube.md instructions.","sidebar":"guideSidebar"},"intro":{"id":"intro","title":"Getting Started","description":"Fork and Clone the Repository"}}}}')}}]);