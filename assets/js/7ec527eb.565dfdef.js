"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[4055],{456:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"architecture/Components/kvcache","title":"KVCache Manager","description":"Introduction","source":"@site/docs/architecture/Components/06_kvcache.md","sourceDirName":"architecture/Components","slug":"/architecture/Components/kvcache","permalink":"/llm-d.github.io/docs/architecture/Components/kvcache","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidecar_position":6,"sidecar_label":"KVCache Manager"},"sidebar":"structureSidebar","previous":{"title":"Routing Sidecar","permalink":"/llm-d.github.io/docs/architecture/Components/routing-sidecar"}}');var i=r(4848),o=r(8453);const s={sidecar_position:6,sidecar_label:"KVCache Manager"},a="KVCache Manager",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Overview",id:"overview",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Detailed System Flow",id:"detailed-system-flow",level:3},{value:"Explanation",id:"explanation",level:3},{value:"Maintenance of Redis for KVBlock -&gt; Pods Mapping",id:"maintenance-of-redis-for-kvblock---pods-mapping",level:3},{value:"Examples",id:"examples",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"kvcache-manager",children:"KVCache Manager"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"LLM inference can be computationally expensive due to the sequential nature of token generation.\r\nKV-caching plays a critical role in optimizing this process. By storing previously computed key and value attention vectors,\r\nKVCache reuse avoids redundant computations during inference, significantly reducing latency and resource consumption.\r\nThis is particularly beneficial for long context multi-turn conversations or Agentic (&RAG) applications where\r\npreviously computed information can be leveraged effectively.\r\nEfficient KVCache management and routing are essential for scaling LLM inference and delivering a responsive user experience."}),"\n",(0,i.jsx)(n.p,{children:"llmd-kv-cache-manager is a pluggable KVCache Manager for KVCache Aware Routing in vLLM-based serving platforms."}),"\n",(0,i.jsxs)(n.p,{children:["See the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-kv-cache-manager/blob/main/docs/README.md",children:"docs folder in the repository"})," for more information on goals, architecture and more."]}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The code defines a ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-kv-cache-manager/tree/main/pkg/kv-cache/indexer.go",children:"KVCacheIndexer"})," module that efficiently maintains a global view of KVCache states and localities.\r\nIn the current state of vLLM, the only available information on KVCache availability is that of the offloaded tensors to KVCache Engines via the Connector API."]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"kvcache.Indexer"})," module is a pluggable Go package designed for use by orchestrators to enable KVCache-aware scheduling decisions."]}),"\n",(0,i.jsx)(n.mermaid,{value:'graph \r\n  subgraph Cluster\r\n    Router\r\n    subgraph KVCacheManager[KVCache Manager]\r\n      KVCacheIndexer[KVCache Indexer]\r\n      PrefixStore[LRU Prefix Store]\r\n      KVBlockToPodIndex[KVBlock to Pod availability Index]\r\n    end\r\n    subgraph vLLMNode[vLLM Node]\r\n      vLLMCore[vLLM Core]\r\n      KVCacheEngine["KVCache Engine (LMCache)"]\r\n    end\r\n    Redis\r\n  end\r\n\r\n  Router --\x3e|"Score(prompt, ModelName, relevantPods)"| KVCacheIndexer\r\n  KVCacheIndexer --\x3e|"{Pod to Scores map}"| Router\r\n  Router --\x3e|Route| vLLMNode\r\n  \r\n  KVCacheIndexer --\x3e|"FindLongestTokenizedPrefix(prompt, ModelName) -> tokens"| PrefixStore\r\n  PrefixStore --\x3e|"DigestPromptAsync"| PrefixStore\r\n  KVCacheIndexer --\x3e|"GetPodsForKeys(tokens) -> {KVBlock keys to Pods} availability map"| KVBlockToPodIndex\r\n  KVBlockToPodIndex --\x3e|"Redis MGet(blockKeys) -> {KVBlock keys to Pods}"| Redis\r\n\r\n  vLLMCore --\x3e|Connector API| KVCacheEngine\r\n  KVCacheEngine --\x3e|"UpdateIndex(KVBlock keys, nodeIP)"| Redis'}),"\n",(0,i.jsx)(n.p,{children:"This overview greatly simplifies the actual architecture and combines steps across several submodules."}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsxs)(n.p,{children:["For even more a detailed architecture, refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-kv-cache-manager/tree/main/docs/architecture.md",children:"architecture"})," document."]}),"\n",(0,i.jsx)(n.p,{children:"The architecture is designed to efficiently maintain a global view of KVCache states and localities, enabling KVCache-aware scheduling decisions."}),"\n",(0,i.jsx)(n.h3,{id:"detailed-system-flow",children:"Detailed System Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\r\n    participant U as User  \r\n    participant KVI as KVCacheIndexer\r\n    box\r\n        participant KVBS as KVBlockScorer\r\n        participant TPR as TokenProcessor\r\n        participant KVBI as KVBlockIndexer\r\n        participant Redis as Redis\r\n    end\r\n    box\r\n        participant PS as PrefixStore\r\n        participant LRUS as LRUStore\r\n        participant TS as TrieStore\r\n    end\r\n    box\r\n        participant TPO as TokenizersPool\r\n        participant W as Worker\r\n        participant CHT as HuggingFaceTokenizer\r\n        participant CH as TokenizersCache\r\n    end\r\n\r\n# KVCacheIndexer\r\nU->>KVI: 1. Score(prompt, ModelName, relevantPods)\r\n\r\n# get available tokens of longest prefix\r\nKVI->>PS: 2. FindLongestTokenizedPrefix(prompt, ModelName)\r\n    alt LRU\r\n        PS->>LRUS: 2.1 BuildLongestPrefix(prompt, ModelName)\r\n    else Trie\r\n        PS->>TS: 2.1 BuildLongestPrefix(prompt, ModelName)\r\n    end\r\nPS->>KVI: 2.2 Tokens of longest prefix\r\n\r\n# get block keys  \r\nKVI->>TPR: 3 GetBlockKeys(tokens, ModelName)\r\n    TPR->>KVI: 3.1 BlockKeys\r\n\r\n# query kvblock indexer for pods\r\nKVI->>KVBI: 4. GetPodsForKeys(blockKeys, relevantPods)\r\nKVBI->>Redis: 4.1 MGet(blockKeys)\r\nRedis->>KVBI: 4.2 key -> Pods mapping (KVCache availability)\r\nKVBI->>KVBI: 4.3 FilterPods(relevantPods)\r\n\r\n# score pods\r\nKVI->>KVBS: 5. ScorePods(key->Pods) based on strategy\r\n\r\n# results\r\nKVI->>U: 6. Pod -> Score mapping\r\n\r\n# add to tokenizers pool\r\nKVI->>TPO: 2. AddTask(prompt, ModelName) // Registers task only\r\nNote over TPO: Task added to queue\r\nW--\x3e>TPO: 2.1 Get(Task) // Async worker fetches task\r\nW->>CHT: 2.3 Tokenize(prompt, ModelName)\r\nCHT->>CH: 2.4 GetCachedTokenizerForModel()\r\nCHT->>W: 2.5 Tokens\r\nW->>PS: 2.6 AddTokens(prompt, ModelName, tokens)\r\nalt LRU\r\n    PS->>LRUS: 2.7 AddTokens(prompt, ModelName, tokens)\r\nelse Trie\r\n    PS->>TS: 2.7 AddTokens(prompt, ModelName, tokens)\r\nend"}),"\n",(0,i.jsx)(n.h3,{id:"explanation",children:"Explanation"}),"\n",(0,i.jsx)(n.p,{children:"The main blocking sequence of steps that happens when a user (e.g., router) sends a request to the KVCacheIndexer is as follows:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"User"})," sends a request to the ",(0,i.jsx)(n.strong,{children:"KVCacheIndexer"})," with a prompt, model name, and relevant pods."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KVCacheIndexer"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Finds the longest tokenized prefix for the prompt and model name using the ",(0,i.jsx)(n.strong,{children:"PrefixStore"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Depending on the store type (LRU or Trie), it gets the tokenization of the longest cached prefix"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Adds a tokenization task to the ",(0,i.jsx)(n.strong,{children:"TokenizersPool"}),", which is handled asynchronously by a worker. This bit is explained later."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KVCacheIndexer"})," queries the ",(0,i.jsx)(n.strong,{children:"TokenProcessor"})," to get block keys for the tokens of the longest prefix."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"TokenProcessor"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Chunks the tokens and generate keys for the token blocks. The chunking and key calculating has to be aligned with\r\nthe source that feeds the key -> pods backend (Redis)."}),"\n",(0,i.jsxs)(n.li,{children:["Returns the block keys to the ",(0,i.jsx)(n.strong,{children:"KVCacheIndexer"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KVCacheIndexer"})," queries the ",(0,i.jsx)(n.strong,{children:"KVBlockIndexer"})," for pods that have the block keys.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.strong,{children:"KVBlockIndexer"})," queries the ",(0,i.jsx)(n.strong,{children:"Redis"})," backend for the mappings with MGet."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.strong,{children:"Redis"})," backend efficiently returns the key -> pods mapping."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KVCacheIndexer"})," uses the configured ",(0,i.jsx)(n.strong,{children:"KVBlockScorer"})," to score the pods based block hits:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"LongestPrefixMatch: scores by the longest consecutive (ordered) block hits in a single pod."}),"\n",(0,i.jsx)(n.li,{children:"HighestBlockHit: scores by the index of the highest block hit in a single pod."}),"\n",(0,i.jsx)(n.li,{children:"CoverageBasedMatching: scores by the total number of block hits in a single pod."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Asynchronous tokenization flow:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["A worker fetches the task from the ",(0,i.jsx)(n.strong,{children:"TokenizersPool"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["The worker tokenizes the prompt using the ",(0,i.jsx)(n.strong,{children:"HuggingFaceTokenizer"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.strong,{children:"HuggingFaceTokenizer"})," retrieves the cached in-memory tokenizer for the model.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"If the tokenizer is not cached, it gets created and cached."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.strong,{children:"HuggingFaceTokenizer"})," returns the tokens to the worker."]}),"\n",(0,i.jsxs)(n.li,{children:["The worker adds the tokens to the ",(0,i.jsx)(n.strong,{children:"PrefixStore"}),".","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Depending on the store type (LRU or Trie), it adds the tokens to the appropriate store:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"LRUStore: an LRU HashTable of prompt-chunks to tokens"}),"\n",(0,i.jsx)(n.li,{children:"TrieStore: a Trie of characters to tokens"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Due to the nature of how tokenizers operate, the tokenization of a prefix of a prompt is a prefix of the tokenization of the full prompt.\r\nOne challenge in tokenization is that different chunks of a prompt map to different tokens.\r\nTherefore, when we chunk a prompt, we use the [_, end] index associated with the tokens to contain token in a chunk.\r\nThe implication of this design is that the tokens contained in a chunk are only correct if all previous chunks are also considered,\r\nsince one token may be associated with the edge-characters of two consecutive chunks."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"maintenance-of-redis-for-kvblock---pods-mapping",children:"Maintenance of Redis for KVBlock -> Pods Mapping"}),"\n",(0,i.jsx)(n.p,{children:"In the current phase, LMCache is set up to use the same Redis server for indexing. For the scope of LMCache, this indexing\r\nis necessary for KVCache reuse through offloading and sharing."}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-kv-cache-manager/tree/main/examples/kv-cache-index/",children:"KVCache Indexer"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A reference implementation of using the ",(0,i.jsx)(n.code,{children:"kvcache.Indexer"})," module."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-kv-cache-manager/tree/main/examples/kv-cache-aware-scorer/",children:"KVCache Aware Scorer"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["A reference implementation of integrating the ",(0,i.jsx)(n.code,{children:"kvcache.Indexer"})," module in\r\n",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-inference-scheduler",children:"llm-d-inference-scheduler"})," in a KVCache aware scorer."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);