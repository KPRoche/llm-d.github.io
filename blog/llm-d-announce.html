<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Announcing the llm-d community! | llm-d</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://llm-d.ai/llm-d.github.io/blog/llm-d-announce"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Announcing the llm-d community! | llm-d"><meta data-rh="true" name="description" content="Debut announcement of llm-d project and community"><meta data-rh="true" property="og:description" content="Debut announcement of llm-d project and community"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-05-20T08:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/robertgshaw2-redhat,https://github.com/smarterclayton,https://github.com/chcost"><meta data-rh="true" property="article:tag" content="Hello,Welcome!,llm-d release news!"><link data-rh="true" rel="icon" href="/llm-d.github.io/img/llm-d-favicon.png"><link data-rh="true" rel="canonical" href="https://llm-d.ai/llm-d.github.io/blog/llm-d-announce"><link data-rh="true" rel="alternate" href="https://llm-d.ai/llm-d.github.io/blog/llm-d-announce" hreflang="en"><link data-rh="true" rel="alternate" href="https://llm-d.ai/llm-d.github.io/blog/llm-d-announce" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://llm-d.ai/llm-d.github.io/blog/llm-d-announce","mainEntityOfPage":"https://llm-d.ai/llm-d.github.io/blog/llm-d-announce","url":"https://llm-d.ai/llm-d.github.io/blog/llm-d-announce","headline":"Announcing the llm-d community!","name":"Announcing the llm-d community!","description":"Debut announcement of llm-d project and community","datePublished":"2025-05-20T08:00:00.000Z","author":[{"@type":"Person","name":"Robert Shaw","description":"Director of Engineering, Red Hat","url":"https://github.com/robertgshaw2-redhat","email":"robshaw@redhat.com","image":"https://avatars.githubusercontent.com/u/114415538?v=4"},{"@type":"Person","name":"Clayton Coleman","description":"Distinguished Engineer, Google","url":"https://github.com/smarterclayton","email":"claytoncoleman@google.com","image":"https://avatars.githubusercontent.com/u/1163175?v=4"},{"@type":"Person","name":"Carlos Costa","description":"Distinguished Engineer, IBM","url":"https://github.com/chcost","email":"chcost@us.ibm.com","image":"https://avatars.githubusercontent.com/u/26551701?v=4"}],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://llm-d.ai/llm-d.github.io/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/llm-d.github.io/blog/rss.xml" title="llm-d RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/llm-d.github.io/blog/atom.xml" title="llm-d Atom Feed"><link rel="stylesheet" href="/llm-d.github.io/assets/css/styles.d0841e62.css">
<script src="/llm-d.github.io/assets/js/runtime~main.91fe2986.js" defer="defer"></script>
<script src="/llm-d.github.io/assets/js/main.5e242a3c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/llm-d.github.io/img/llm-d-icon.png"><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/114415538?v=4"><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/1163175?v=4"><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/26551701?v=4"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/llm-d.github.io/"><div class="navbar__logo"><img src="/llm-d.github.io/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/llm-d.github.io/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/architecture/architecture">What is llm-d?</a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/guide">User Guide</a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/community/contribute">Community</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/llm-d.github.io/blog">News</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/llm-d/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link github"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/llm-d.github.io/blog/llm-d-announce">Announcing the llm-d community!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/llm-d.github.io/blog/llm-d-press-release">llm-d Press Release</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Announcing the llm-d community!</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-05-20T08:00:00.000Z">May 20, 2025</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/robertgshaw2-redhat" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/114415538?v=4" alt="Robert Shaw"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/robertgshaw2-redhat" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Robert Shaw</span></a></div><small class="authorTitle_nd0D" title="Director of Engineering, Red Hat">Director of Engineering, Red Hat</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/smarterclayton" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/1163175?v=4" alt="Clayton Coleman"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/smarterclayton" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Clayton Coleman</span></a></div><small class="authorTitle_nd0D" title="Distinguished Engineer, Google">Distinguished Engineer, Google</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/chcost" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://avatars.githubusercontent.com/u/26551701?v=4" alt="Carlos Costa"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/chcost" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Carlos Costa</span></a></div><small class="authorTitle_nd0D" title="Distinguished Engineer, IBM">Distinguished Engineer, IBM</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="announcing-the-llm-d-community">Announcing the llm-d community<a href="#announcing-the-llm-d-community" class="hash-link" aria-label="Direct link to Announcing the llm-d community" title="Direct link to Announcing the llm-d community">​</a></h2>
<p>llm-d is a Kubernetes-native high-performance distributed LLM inference framework<br>
<!-- -->- a well-lit path for anyone to serve at scale, with the fastest time-to-value and competitive performance per dollar for most models across most hardware accelerators.</p>
<p>With llm-d, users can operationalize gen AI deployments with a modular, high-performance, end-to-end serving solution that leverages the latest distributed inference optimizations like KV-cache aware routing and disaggregated serving, co-designed and integrated with the Kubernetes operational tooling in <a href="https://github.com/kubernetes-sigs/gateway-api-inference-extension?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">Inference Gateway (IGW)</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-inference-goes-distributed">LLM Inference Goes Distributed<a href="#llm-inference-goes-distributed" class="hash-link" aria-label="Direct link to LLM Inference Goes Distributed" title="Direct link to LLM Inference Goes Distributed">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-standard-scale-out-falls-short">Why Standard Scale Out Falls Short<a href="#why-standard-scale-out-falls-short" class="hash-link" aria-label="Direct link to Why Standard Scale Out Falls Short" title="Direct link to Why Standard Scale Out Falls Short">​</a></h4>
<p>Kubernetes typically scales out application workloads with uniform replicas and round-robin load balancing.</p>
<p><img decoding="async" loading="lazy" alt="Figure 1: Deploying a service to multiple vLLM instances" src="/llm-d.github.io/assets/images/image5_46-4c97fed067d33b3eeaf484aedd38777e.png" width="4605" height="3122" class="img_ev3q"></p>
<p>This simple pattern is very effective for most request patterns, which have the following characteristics:</p>
<ul>
<li>Requests are short-lived and generally uniform in resource utilization</li>
<li>Requests have generally uniform latency service level objectives (SLOs)</li>
<li>Each replica can process each request equally well</li>
<li>Specializing variants and coordinating replicas to process a single request is not useful</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llm-serving-is-unique">LLM Serving Is Unique<a href="#llm-serving-is-unique" class="hash-link" aria-label="Direct link to LLM Serving Is Unique" title="Direct link to LLM Serving Is Unique">​</a></h4>
<p>The LLM inference workload, however, is unique with slow, non-uniform, expensive requests. This means that typical scale-out and load-balancing patterns fall short of optimal performance.</p>
<p><img decoding="async" loading="lazy" alt="Figure 2: Comparison of modern HTTP requests" src="/llm-d.github.io/assets/images/image7_33-22569159e4120833431afcdb6f74a6c0.png" width="1768" height="726" class="img_ev3q"></p>
<p>Let’s take a look at each one step-by-step:</p>
<p><em>A. Requests are expensive with significant variance in resource utilization.</em></p>
<ul>
<li>Each LLM inference request has a different “shape” to it, as measured by the number of input tokens and output tokens. There is significant variance in these parameters across requests and workloads.<!-- -->
<ul>
<li>RAG has long inputs - prompt and retrieved docs - and short generated outputs</li>
<li>Reasoning has a short or medium inputs and long generated outputs</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="Figure 3: Comparing the RAG pattern and Thinking/Reasoning pattern with prefill and decode stages" src="/llm-d.github.io/assets/images/image2_4-659d9ff985978b910ecfab7e29e8024b.jpg" width="1999" height="741" class="img_ev3q"></p>
<ul>
<li>These differences in request times can lead to significant imbalances across instances, which are compounded as loaded instances get overwhelmed. Overloads lead to longer ITL (Inter-Token Latency), which leads to more load, which leads to more ITL.</li>
</ul>
<p><em>B. Routing to specific replicas with cached prior computation can achieve orders of magnitude better latency.</em></p>
<ul>
<li>Many common LLM workloads have “multi-turn” request patterns, where the same prompt is sent iteratively to the same instance.<!-- -->
<ul>
<li>Agentic (tool calls are iterative request flow)</li>
<li>Code completion task (requests reuse current codebase as context)</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="The agentic pattern sequence" src="/llm-d.github.io/assets/images/image8_0-58f2529f00c9948aeab638a3b0cef7e5.jpg" width="1999" height="722" class="img_ev3q"></p>
<ul>
<li>LLM inference servers like vLLM implement a method called “automatic prefix caching”, which enables “skipping” a significant amount of prefill computation when there is a cache hit. If requests are routed to vLLM replicas that have the data in the cache, we skip computation. Increasing the likelihood of prefix cache hits with a larger cache size can dramatically improve tail latencies.</li>
</ul>
<p><img decoding="async" loading="lazy" alt="The prefix aching method" src="/llm-d.github.io/assets/images/image3-3cd6d74c02dfdc443896dbd059d774cd.jpg" width="1999" height="1744" class="img_ev3q"></p>
<p><em>C. Specializing and coordinating replicas to process a single request can lead to more throughput per GPU.</em></p>
<ul>
<li>
<p>Inference is split into two phases – prefill and decode.  Prefill generates the first output token and runs in parallel over all the prompt tokens - this phase is compute bound.  Decode generates tokens one at a time by doing a full pass over the model, making this phase memory bandwidth-bound.</p>
</li>
<li>
<p>Standard LLM deployments perform the prefill and decode phases of inference within a single replica.Given that prefill and decode phases of inference have different resource requirements, co-locating these phases on the same replica leads to inefficient resource use, especially for long sequences.</p>
</li>
<li>
<p><strong>Disaggregation</strong> (e.g. <a href="https://arxiv.org/abs/2401.09670" target="_blank" rel="noopener noreferrer">Distserve</a>) separates prefill and decode phases onto different variants, enabling independent optimization and scaling of each phase.</p>
<ul>
<li>
<p>Google <a href="https://cloud.google.com/blog/products/compute/whats-new-with-ai-hypercomputer" target="_blank" rel="noopener noreferrer">leverages disaggregated serving on TPUs</a> to provide better first-token latency and simplify operational scaling.</p>
</li>
<li>
<p>DeepSeek released a <a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md" target="_blank" rel="noopener noreferrer">discussion of the design of their inference system</a>, which leverages aggressive disaggregation to achieve remarkable performance at scale.</p>
</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="Disaggregation separates the prefill and decode phases" src="/llm-d.github.io/assets/images/image4_57-2bd96d01f1faa556ac64ed77fe0867b7.png" width="1999" height="1144" class="img_ev3q"></p>
<p><em>D. Production deployments often have a range of quality of service (QoS) requirements.</em></p>
<ul>
<li>
<p>Use cases for a single LLM endpoint can have a wide variety of quality of service requirements. Consider the following examples:</p>
<ul>
<li>Latency is the most important factor: Code completion requests and search responses need to minimize latency to provide an “in the loop” experience. O(ms) latency tolerance.</li>
<li>Latency is important: Chat agent sessions and email drafting with interactive use cases. O(seconds) latency tolerance.</li>
<li>Latency tolerant: Video call and email summarization and “deep research” agents with daily or hourly usage patterns. O(minutes) latency tolerance.</li>
<li>Latency agnostic: Overnight batch processing workloads, meeting minute generation, and autonomous agents. O(hours) latency tolerance.</li>
</ul>
</li>
<li>
<p>Given the compute intensity (and, therefore, high costs) of LLMs, tight latency SLOs are substantially more expensive to achieve. This spectrum of latency requirements presents an opportunity to further optimize infrastructure efficiency – the more latency tolerant a workload is, the more we can optimize infrastructure efficiency amongst other workloads.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-llm-d">Why llm-d?<a href="#why-llm-d" class="hash-link" aria-label="Direct link to Why llm-d?" title="Direct link to Why llm-d?">​</a></h3>
<p>To exploit these characteristics and achieve optimal performance for LLM workloads, the inference serving landscape is rapidly transitioning towards distributed cluster-scale architectures. For instance, in its “Open Source Week”, the DeepSeek team published the design of its <a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md" target="_blank" rel="noopener noreferrer">inference system</a>, which aggressively leverages disaggregation and KV caching to achieve remarkable performance per $ of compute.</p>
<p>However, for most GenAI innovators, ML platform teams, and IT operations groups, these benefits remain out of reach. Building and operating a complex, monolithic system is time-consuming and challenging, especially in the context of the rapid pace of innovation and enterprise deployments with tens or hundreds of models for divergent use cases. This complexity risks time to market, higher operational costs and sprawl, and difficulty adopting and experimenting.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="our-objective">Our Objective<a href="#our-objective" class="hash-link" aria-label="Direct link to Our Objective" title="Direct link to Our Objective">​</a></h4>
<p>The objective of llm-d is to create a well-lit path for anyone to adopt the leading distributed inference optimizations <em>within their existing deployment framework</em> - Kubernetes.</p>
<p>To achieve this goal, we have the following design principles for the project:</p>
<ul>
<li><strong>Operationalizability:</strong> modular and resilient architecture with native integration into Kubernetes via Inference Gateway API</li>
<li><strong>Flexibility:</strong> cross-platform (active work to support NVIDIA, Google TPU, AMD, and Intel), with extensible implementations of key composable layers of the stack</li>
<li><strong>Performance</strong>: leverage distributed optimizations like disaggregation and prefix-aware routing to achieve the highest tok/$ while meeting SLOs</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h4>
<p>To achieve this objective, we designed llm-d with a modular and layered architecture on top of industry-standard open-source technologies - vLLM, Kubernetes, and Inference Gateway.</p>
<ul>
<li>
<p><a href="https://docs.vllm.ai/en/latest/" target="_blank" rel="noopener noreferrer"><strong>vLLM</strong>. vLLM</a> is the leading open-source LLM inference engine, supporting a wide range of models (including Llama and DeepSeek) and hardware accelerators (including NVIDIA GPU, Google TPU, AMD ) with high performance.</p>
</li>
<li>
<p><a href="https://kubernetes.io/docs/home/" target="_blank" rel="noopener noreferrer"><strong>Kubernetes</strong></a> <strong>(K8s)</strong>. K8s is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. It is the industry standard for deploying and updating LLM inference engines across various hardware accelerators.</p>
</li>
<li>
<p><a href="https://gateway-api-inference-extension.sigs.k8s.io/" target="_blank" rel="noopener noreferrer"><strong>Inference Gateway</strong></a> <strong>(IGW)</strong>. IGW is an official Kubernetes project that extends the <a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener noreferrer">Gateway API</a> (the next generation of Kubernetes Ingress and Load Balancing API) with inference-specific routing. IGW includes many important features like model routing, serving priority, and extensible scheduling logic for “smart” load balancing. IGW integrates with many different gateway implementations, such as Envoy, making it widely portable across Kubernetes clusters.</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" src="/llm-d.github.io/assets/images/llm-d-arch-simplified-d41875ab8b1fcf94a1a42df44940ceae.svg" width="859" height="474" class="img_ev3q"></p>
<p>And our key new contributions:</p>
<ul>
<li>
<p><strong>vLLM Optimized Inference Scheduler</strong> - IGW defines a pattern for customizable “smart” load-balancing via the <a href="https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/004-endpoint-picker-protocol" target="_blank" rel="noopener noreferrer">Endpoint Picker Protocol (EPP)</a>. Leveraging enhanced operational telemetry exposed by vLLM, the inference scheduler implements the filtering and scoring algorithms necessary to make “smart” scheduling decisions around disaggregated serving, prefix-cache-awareness, and load-awareness, validated to be used out-of-the-box by llm-d users. Advanced teams can also tweak or implement their own scorers and filterers to further customize for their use cases, while still benefiting from upcoming operational features in the inference gateway, like flow control and latency-aware balancing.</p>
<ul>
<li>For more details, see our Northstar: <a href="https://docs.google.com/document/d/1kE1LY8OVjiOgKVD9-9Po96HODbTIbgHp4qgvw06BCOc/edit?tab=t.0" target="_blank" rel="noopener noreferrer">[PUBLIC] llm-d Scheduler Northstar</a></li>
</ul>
</li>
<li>
<p><strong>Disaggregated Serving with <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> -</strong> llm-d leverages vLLM’s recently enabled support for disaggregated serving via a pluggable KV Connector API to run prefill and decode on independent instances, using high-performance transport libraries like <a href="https://github.com/ai-dynamo/nixl" target="_blank" rel="noopener noreferrer">NVIDIA’s NIXL</a>.</p>
<p>In llm-d, we plan to support two “well-lit” paths for prefill/decode (P/D) disaggregation:</p>
<ul>
<li>Latency optimized implementation using fast interconnects (IB, RDMA, ICI)</li>
<li>Throughput optimized implementation using data center networking</li>
<li>For more details, see our Northstar:<a href="https://docs.google.com/document/d/1FNN5snmipaTxEA1FGEeSH7Z_kEqskouKD1XYhVyTHr8/edit?tab=t.0#heading=h.ycwld2oth1kj" target="_blank" rel="noopener noreferrer">[PUBLIC] llm-d Disaggregated Serving Northstar</a></li>
</ul>
</li>
<li>
<p><strong>Disaggregated Prefix Caching with vLLM</strong> -  llm-d uses the same vLLM KV connector API used in disaggregated serving to provide a pluggable cache for previous calculations, including offloading KVs to host, remote storage, and systems like <a href="https://github.com/LMCache/LMCache" target="_blank" rel="noopener noreferrer">LMCache</a>.</p>
<p>In llm-d, we plan to support two “well-lit” paths for KV cache disaggregation:</p>
<ul>
<li>Independent caching with basic offloading to host memory and disk, providing a zero operational cost mechanism that utilizes all system resources</li>
<li>Shared caching with KV transfer between instances and shared storage with global indexing, providing potential for higher performance at the cost of a more operationally complex system.</li>
<li>For more details, see our Northstar: <a href="https://docs.google.com/document/d/1d-jKVHpTJ_tkvy6Pfbl3q2FM59NpfnqPAh__Uz_bEZ8/edit?tab=t.0#heading=h.6qazyl873259" target="_blank" rel="noopener noreferrer">[PUBLIC] llm-d Prefix Caching Northstar</a></li>
</ul>
</li>
<li>
<p><strong>Variant Autoscaling over Hardware, Workload, and Traffic</strong> - Accelerator hardware varies dramatically in terms of compute, memory, and cost, workloads sharing the same models vary by their required quality of service, the distinct phases of LLM inference and large mixture-of-expert models vary on whether they are compute, memory, or network bound, and incoming traffic varies over time and by workload. Today, all of these decisions are made at deployment time, and almost all deployers struggle to enable autoscaling to reduce their costs safely.</p>
<p>Drawing on extensive experience from end users and OSS collaborators like AIBrix, we plan to implement a traffic- and hardware-aware autoscaler that:</p>
<ul>
<li>Measures the capacity of each model server instance</li>
<li>Derive a load function that takes into account different request shapes and QoS</li>
<li>Using the recent traffic mix - QPS (Queries Per Second), QoS, and shape distribution - calculate the optimal mix of instances to handle prefill, decode, and latency-tolerant requests, and label each instance with a grouping</li>
<li>Report load metrics per grouping that allows Kubernetes horizontal pod autoscaling to match hardware in use to hardware needed without violating SLOs</li>
<li>For more details, see our Northstar: <a href="https://docs.google.com/document/d/1inTneLEZTv3rDEBB9KLOB9K6oMq8c3jkogARJqdt_58/edit?tab=t.0" target="_blank" rel="noopener noreferrer">[PUBLIC] llm-d Autoscaling Northstar</a></li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="example-llm-d-features">Example llm-d Features<a href="#example-llm-d-features" class="hash-link" aria-label="Direct link to Example llm-d Features" title="Direct link to Example llm-d Features">​</a></h4>
<p>llm-d integrates IGW and vLLM together, enabling a high performance distributed serving stack. Let’s discuss some of the example features enabled by llm-d.</p>
<p><strong>Prefix and KV cache-aware routing</strong></p>
<p>The first key collaboration between IGW and vLLM in llm-d was developing prefix-cache aware routing to complement the existing KV cache utilization aware load balancing in IGW.</p>
<p>We conducted a series of experiments to evaluate the performance of the <a href="https://github.com/llm-d/llm-d-inference-scheduler" target="_blank" rel="noopener noreferrer">llm-d-inference-scheduler</a> with prefix-aware routing on 2 NVIDIA 8xH100 nodes using the <a href="https://github.com/LMCache/LMBenchmark/tree/main/synthetic-multi-round-qa" target="_blank" rel="noopener noreferrer">LMbenchmark in a long-input/short-output configuration designed</a> to stress KV cache reuse and routing decision quality.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Model</th><th style="text-align:left">Configuration</th><th style="text-align:left">ISL</th><th style="text-align:left">OSL</th><th style="text-align:left">Latency SLO</th></tr></thead><tbody><tr><td style="text-align:left"><strong>S1</strong></td><td style="text-align:left">LlaMA 4 Scout FP8</td><td style="text-align:left">TP2, 2 replicas</td><td style="text-align:left">20,000</td><td style="text-align:left">100</td><td style="text-align:left">None</td></tr><tr><td style="text-align:left"><strong>S2</strong></td><td style="text-align:left">LlaMA 4 Scout FP8</td><td style="text-align:left">TP2, 4 replicas</td><td style="text-align:left">12,000</td><td style="text-align:left">100</td><td style="text-align:left">P95 TTFT &lt;= 2s</td></tr><tr><td style="text-align:left"><strong>S3</strong></td><td style="text-align:left">Llama 3.1 70B FP16</td><td style="text-align:left">TP2, 4 replicas</td><td style="text-align:left">8,000</td><td style="text-align:left">100</td><td style="text-align:left">P95 TTFT &lt;= 2s</td></tr></tbody></table>
<h1><img decoding="async" loading="lazy" src="/llm-d.github.io/assets/images/image1_116-bdf665a6160ab331bf11d8bee505d7f3.png" width="1999" height="1415" class="img_ev3q"></h1>
<p><strong>Key Observations:</strong></p>
<ul>
<li><strong>S1:</strong> At 4 QPS, llm-d achieves a mean TTFT approximately 3X lower than the baseline (lower is better).</li>
<li><strong>S2:</strong> llm-d delivers ~50% higher QPS than the baseline while meeting SLO requirements (higher is better).</li>
<li><strong>S3:</strong> llm-d sustains 2X the baseline QPS under SLO constraints (higher is better).</li>
</ul>
<p>These results show that llm-d’s cache- and prefix-aware scheduling effectively reduces TTFT and increases QPS compared to the baseline, while consistently meeting SLA requirements.</p>
<p>Try it out with the `base.yaml` config in our <a href="https://github.com/llm-d/llm-d-deployer/tree/main/quickstart" target="_blank" rel="noopener noreferrer">quickstart</a>. And as a customization example, see the  <a href="https://github.com/llm-d/llm-d-inference-scheduler/blob/main/docs/create_new_filter.md" target="_blank" rel="noopener noreferrer">template</a> for adding your own scheduler filter.</p>
<p><strong>P/D disaggregation</strong></p>
<p>We’ve completed an initial implementation of P/D disaggregation with vLLM and llm-d-inference-scheduler, which delivers promising speedups for prefill-heavy workloads (20:1 ISL | OSL). Our next focus is finalizing the implementation with heterogeneous TP and completing comprehensive benchmarks for disaggregated serving. Short-term priorities include enabling heterogeneous TP, scaling with high-performance P/D + EP&lt;&gt;DP for large scale MoEs, and DP-aware load balancing. We will follow up with a detailed performance blog in the coming weeks.</p>
<p>Try it out with the pd-nixl.yaml config in our <a href="https://github.com/llm-d/llm-d-deployer/tree/main/quickstart" target="_blank" rel="noopener noreferrer">quickstart</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="get-started-with-llm-d">Get started with llm-d<a href="#get-started-with-llm-d" class="hash-link" aria-label="Direct link to Get started with llm-d" title="Direct link to Get started with llm-d">​</a></h3>
<p>llm-d builds brings together the performance of vLLM with the operationalizability of Kuberentes, creating a modular architecture for distributed LLM inference, targeting high performance on the latest models and agentic architectures.</p>
<p>We welcome AI engineers and researchers to join the llm-d community and contribute:</p>
<ul>
<li>Check out our repository on Github: <a href="https://github.com/llm-d/llm-d" target="_blank" rel="noopener noreferrer">https://github.com/llm-d/llm-d</a></li>
<li>Join our developer slack: <a href="https://inviter.co/llm-d-slack" target="_blank" rel="noopener noreferrer">https://inviter.co/llm-d-slack</a></li>
<li>Try out our quick starts to deploy llm-d on your Kubernetes cluster: <a href="https://github.com/llm-d/llm-d-deployer/tree/main/quickstart" target="_blank" rel="noopener noreferrer">https://github.com/llm-d/llm-d-deployer/tree/main/quickstart</a></li>
</ul>
<p>Please join us. The future of AI is open.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Hello tag description" class="tag_zVej tagRegular_sFm0" href="/llm-d.github.io/blog/tags/hello">Hello</a></li><li class="tag_QGVx"><a title="Welcome tag description" class="tag_zVej tagRegular_sFm0" href="/llm-d.github.io/blog/tags/welcome">Welcome!</a></li><li class="tag_QGVx"><a title="llm-d tag description" class="tag_zVej tagRegular_sFm0" href="/llm-d.github.io/blog/tags/llm-d">llm-d release news!</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/llm-d.github.io/blog/llm-d-press-release"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">llm-d Press Release</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#announcing-the-llm-d-community" class="table-of-contents__link toc-highlight">Announcing the llm-d community</a><ul><li><a href="#llm-inference-goes-distributed" class="table-of-contents__link toc-highlight">LLM Inference Goes Distributed</a></li><li><a href="#why-llm-d" class="table-of-contents__link toc-highlight">Why llm-d?</a></li><li><a href="#get-started-with-llm-d" class="table-of-contents__link toc-highlight">Get started with llm-d</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">User Guide</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/guide">How to Use</a></li></ul></div><div class="col footer__col"><div class="footer__title">Architecture</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/architecture/architecture">Overview</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/community/contact_us">Contact us</a></li><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/community/contribute">Contributing</a></li><li class="footer__item"><a href="https://github.com/llm-d/llm-d/blob/dev/CODE_OF_CONDUCT.md" target="_blank" rel="noopener noreferrer" class="footer__link-item">Code of Conduct<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/blog">News</a></li><li class="footer__item">
                <a href="https://github.com/llm-d/" target="_blank" rel="noreferrer noopener" aria-label="GitHub Icon">
                  <img class="github-footer" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/github-mark-white.png" alt="GitHub Icon" width="30px" height="auto" margin="5px">
                </a>
              </li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item">
                <a href="https://linkedin.com/company/llm-d" target="_blank" rel="noreferrer noopener" aria-label="LinkedIn Icon">
                  <img class="linkedin" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/linkedin-mark-white.png" alt="LinkedIn Icon" width="30px" height="auto" margin="5px">

                </a>
              </li><li class="footer__item">
                <a href="https://llm-d.slack.com" target="_blank" rel="noreferrer noopener" aria-label="Slack Icon">
                  <img class="slack" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/slack-mark-white.png" alt="Slack Icon" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://inviter.co/llm-d-slack" target="_blank" rel="noreferrer noopener" aria-label="Inviter Icon">
                  <span class="button-link">Join our Slack</span>    
                  <img class="inviter" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/inviter-logo.png" alt="Slack Inviter link" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://www.reddit.com/r/llm_d/" target="_blank" rel="noreferrer noopener" aria-label="Reddit Icon">
                  <img class="reddit" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/reddit-mark-white.png" alt="Reddit Icon" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://x.com/_llm_d_" target="_blank" rel="noreferrer noopener" aria-label="X Icon">
                  <img class="x" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/x-mark-white.png" alt="X Icon" width="30px" height="auto" margin="5px">
                </a>
              </li></ul></div></div></div></footer></div>
</body>
</html>