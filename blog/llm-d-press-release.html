<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">llm-d Press Release | llm-d</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="llm-d Press Release | llm-d"><meta data-rh="true" name="description" content="Official Press Release for llm-d"><meta data-rh="true" property="og:description" content="Official Press Release for llm-d"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-05-20T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="News Releases"><link data-rh="true" rel="icon" href="/llm-d.github.io/img/llm-d-favicon.png"><link data-rh="true" rel="canonical" href="https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release"><link data-rh="true" rel="alternate" href="https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release" hreflang="en"><link data-rh="true" rel="alternate" href="https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release","mainEntityOfPage":"https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release","url":"https://kproche.github.io/llm-d.github.io/blog/llm-d-press-release","headline":"llm-d Press Release","name":"llm-d Press Release","description":"Official Press Release for llm-d","datePublished":"2025-05-20T00:00:00.000Z","author":[],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://kproche.github.io/llm-d.github.io/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/llm-d.github.io/blog/rss.xml" title="llm-d RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/llm-d.github.io/blog/atom.xml" title="llm-d Atom Feed"><link rel="stylesheet" href="/llm-d.github.io/assets/css/styles.d0841e62.css">
<script src="/llm-d.github.io/assets/js/runtime~main.d30533ac.js" defer="defer"></script>
<script src="/llm-d.github.io/assets/js/main.1481be70.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/llm-d.github.io/img/llm-d-icon.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/llm-d.github.io/"><div class="navbar__logo"><img src="/llm-d.github.io/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/llm-d.github.io/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/architecture/architecture">What is llm-d?</a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/guide">User Guide</a><a class="navbar__item navbar__link" href="/llm-d.github.io/docs/community/contribute">Community</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/llm-d.github.io/blog">News</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/llm-d/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link github"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/llm-d.github.io/blog/llm-d-announce">Announcing the llm-d community!</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/llm-d.github.io/blog/llm-d-press-release">llm-d Press Release</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">llm-d Press Release</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-05-20T00:00:00.000Z">May 20, 2025</time> · <!-- -->12 min read</div></header><div id="__blog-post-container" class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="may-20-2025">May 20, 2025<a href="#may-20-2025" class="hash-link" aria-label="Direct link to May 20, 2025" title="Direct link to May 20, 2025">​</a></h2>
<h1><strong>Red Hat Launches the llm-d Community, Powering Distributed Gen AI Inference at Scale</strong></h1>
<p>Forged in collaboration with founding contributors CoreWeave, Google Cloud, IBM Research and NVIDIA and joined by industry leaders AMD, Cisco, Hugging Face, Intel, Lambda and Mistral AI and university supporters at the University of California, Berkeley, and the University of Chicago, the project aims to make production generative AI as omnipresent as Linux</p>
<p><strong>BOSTON – RED HAT SUMMIT – MAY 20, 2025 —</strong> Red Hat, the world&#x27;s leading provider of open source solutions, today announced the launch of llm-d, a new open source project that answers the most crucial need of generative AI’s (gen AI) future: Inference at scale. Tapping breakthrough inference technologies for gen AI at scale, llm-d is powered by a native Kubernetes architecture, vLLM-based distributed inference and intelligent AI-aware network routing, empowering robust, large language model (LLM) inference clouds to meet the most demanding production service-level objectives (SLOs).</p>
<p>While training remains vital, the true impact of gen AI hinges on more efficient and scalable inference - the engine that transforms AI models into actionable insights and user experiences. According to Gartner<sup><a href="#user-content-fn-1-dbc4ec" id="user-content-fnref-1-dbc4ec" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>, “By 2028, as the market matures, more than 80% of data center workload accelerators will be specifically deployed for inference as opposed to training use.” This underscores that the future of gen AI lies in the ability to execute. The escalating resource demands of increasingly sophisticated and larger reasoning models limits the viability of centralized inference and threatens to bottleneck AI innovation with prohibitive costs and crippling latency.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="answering-the-need-for-scalable-gen-ai-inference-with-llm-d">Answering the need for scalable gen AI inference with llm-d<a href="#answering-the-need-for-scalable-gen-ai-inference-with-llm-d" class="hash-link" aria-label="Direct link to Answering the need for scalable gen AI inference with llm-d" title="Direct link to Answering the need for scalable gen AI inference with llm-d">​</a></h3>
<p>Red Hat and its industry partners are directly confronting this challenge with llm-d, a visionary project that amplifies the power of vLLM to transcend single-server limitations and unlock production at scale for AI inference. Using the proven orchestration prowess of Kubernetes, llm-d integrates advanced inference capabilities into existing enterprise IT infrastructures. This unified platform empowers IT teams to meet the diverse serving demands of business-critical workloads, all while deploying innovative techniques to maximize efficiency and dramatically minimize the total cost of ownership (TCO) associated with high-performance AI accelerators.</p>
<p>llm-d delivers a powerful suite of innovations, highlighted by:</p>
<ul>
<li><strong>vLLM</strong>, which has quickly become the open source de facto standard inference server, providing day 0 model support for emerging frontier models, and support for a broad list of accelerators, now including Google Cloud Tensor Processor Units (TPUs).</li>
<li><strong>Prefill and Decode Disaggregation</strong> to separate the input context and token generation phases of AI into discrete operations, where they can then be distributed across multiple servers.</li>
<li><strong>KV (key-value) Cache Offloading</strong>, based on LMCache, shifts the memory burden of the KV cache from GPU memory to more cost-efficient and abundant standard storage, like CPU memory or network storage.</li>
<li><strong>Kubernetes-powered clusters and controllers</strong> for more efficient scheduling of compute and storage resources as workload demands fluctuate, while maintaining performance and lower latency.</li>
<li><strong>AI-Aware Network Routing</strong> for scheduling incoming requests to the servers and accelerators that are most likely to have hot caches of past inference calculations.</li>
<li><strong>High-performance communication APIs</strong> for faster and more efficient data transfer between servers, with support for NVIDIA Inference Xfer Library (NIXL).</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-d-backed-by-industry-leaders">llm-d: Backed by industry leaders<a href="#llm-d-backed-by-industry-leaders" class="hash-link" aria-label="Direct link to llm-d: Backed by industry leaders" title="Direct link to llm-d: Backed by industry leaders">​</a></h3>
<p>This new open source project has already garnered the support of a formidable coalition of leading gen AI model providers, AI accelerator pioneers, and premier AI cloud platforms. CoreWeave, Google Cloud, IBM Research and NVIDIA are founding contributors, with AMD, Cisco, Hugging Face, Intel, Lambda and Mistral AI as partners, underscoring the industry’s deep collaboration to architect the future of large-scale LLM serving. The llm-d community is further joined by founding supporters at the Sky Computing Lab at the University of California, originators of vLLM, and the LMCache Lab at the University of Chicago, originators of <a href="https://github.com/LMCache/LMCache" target="_blank" rel="noopener noreferrer">LMCache</a><em>.</em></p>
<p>Rooted in its unwavering commitment to open collaboration, Red Hat recognizes the critical importance of vibrant and accessible communities in the rapidly evolving landscape of gen AI inference. Red Hat will actively champion the growth of the llm-d community, fostering an inclusive environment for new members and fueling its continued evolution.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="red-hats-vision-any-model-any-accelerator-any-cloud">Red Hat’s vision: Any model, any accelerator, any cloud.<a href="#red-hats-vision-any-model-any-accelerator-any-cloud" class="hash-link" aria-label="Direct link to Red Hat’s vision: Any model, any accelerator, any cloud." title="Direct link to Red Hat’s vision: Any model, any accelerator, any cloud.">​</a></h3>
<p>The future of AI must be defined by limitless opportunity, not constrained by infrastructure silos. Red Hat sees a horizon where organizations can deploy any model, on any accelerator, across any cloud, delivering an exceptional, more consistent user experience without exorbitant costs. To unlock the true potential of gen AI investments, enterprises require a universal inference platform - a standard for more seamless, high-performance AI innovation, both today and in the years to come.</p>
<p>Just as Red Hat pioneered the open enterprise by transforming Linux into the bedrock of modern IT, the company is now poised to architect the future of AI inference. vLLM’s potential is that of a linchpin for standardized gen AI inference, and Red Hat is committed to building a thriving ecosystem around not just the vLLM community but also llm-d for distributed inference at scale. The vision is clear: regardless of the AI model or the underlying accelerator or the deployment environment, Red Hat intends to make vLLM the definitive open standard for inference across the new hybrid cloud.</p>
<p><strong>Red Hat Summit</strong><br>
<!-- -->Join the Red Hat Summit keynotes to hear the latest from Red Hat executives, customers and partners:</p>
<ul>
<li><a href="https://events.experiences.redhat.com/widget/redhat/sum25/SessionCatalog2025/session/1737554802676001HJ8q" target="_blank" rel="noopener noreferrer"><strong>Modernized infrastructure meets enterprise-ready AI</strong></a> — Tuesday, May 20, 8-10 a.m. EDT (<a href="https://youtube.com/live/Gr8jomztY2s?feature=share" target="_blank" rel="noopener noreferrer">YouTube</a>)</li>
<li><a href="https://events.experiences.redhat.com/widget/redhat/sum25/SessionCatalog2025/session/1737554802763001Hr0T" target="_blank" rel="noopener noreferrer"><strong>Hybrid cloud evolves to deliver enterprise innovation</strong></a> — Wednesday, May 21, 8-9:30 a.m. EDT (<a href="https://youtube.com/live/g0K0pJIKHBU?feature=share" target="_blank" rel="noopener noreferrer">YouTube</a>)</li>
</ul>
<p><strong>Supporting Quotes</strong><br>
<em>Brian Stevens, senior vice president and AI CTO, Red Hat</em><br>
<!-- -->“The launch of the llm-d community, backed by a vanguard of AI leaders, marks a pivotal moment in addressing the need for scalable gen AI inference, a crucial obstacle that must be overcome to enable broader enterprise AI adoption. By tapping the innovation of vLLM and the proven capabilities of Kubernetes, llm-d paves the way for distributed, scalable and high-performing AI inference across the expanded hybrid cloud, supporting any model, any accelerator, on any cloud environment and helping realize a vision of limitless AI potential.”</p>
<p><em>Ramine Roane, corporate vice president, AI Product Management, AMD</em><br>
<!-- -->&quot;AMD is proud to be a founding member of the llm-d community, contributing our expertise in high-performance GPUs to advance AI inference for evolving enterprise AI needs. As organizations navigate the increasing complexity of generative AI to achieve greater scale and efficiency, AMD looks forward to meeting this industry demand through the llm-d project.&quot;</p>
<p><em>Shannon McFarland, vice president, Cisco Open Source Program Office &amp; Head of Cisco DevNet</em><br>
<!-- -->“The llm-d project is an exciting step forward for practical generative AI. llm-d empowers developers to programmatically integrate and scale generative AI inference, unlocking new levels of innovation and efficiency in the modern AI landscape. Cisco is proud to be part of the llm-d community, where we’re working together to explore real-world use cases that help organizations apply AI more effectively and efficiently.”</p>
<p><em>Chen Goldberg, senior vice president, Engineering, CoreWeave</em><br>
<!-- -->“CoreWeave is proud to be a founding contributor to the llm-d project and to deepen our long-<br>
<!-- -->standing commitment to open source AI. From our early partnership with EleutherAI to our ongoing work advancing inference at scale, we’ve consistently invested in making powerful AI infrastructure more accessible. We’re excited to collaborate with an incredible group of partners<br>
<!-- -->and the broader developer community to build a flexible, high-performance inference engine<br>
<!-- -->that accelerates innovation and lays the groundwork for open, interoperable AI.”</p>
<p><em>Mark Lohmeyer, vice president and general manager, AI &amp; Computing Infrastructure, Google Cloud</em><br>
<!-- -->&quot;Efficient AI inference is paramount as organizations move to deploying AI at scale and deliver value for their users. As we enter this new age of inference, Google Cloud is proud to build upon our legacy of open source contributions as a founding contributor to the llm-d project. This new community will serve as a critical catalyst for distributed AI inference at scale, helping users realize enhanced workload efficiency with increased optionality for their infrastructure resources.&quot;</p>
<p><em>Jeff Boudier, Head of Product, Hugging Face</em><br>
<!-- -->“We believe every company should be able to build and run their own models. With vLLM leveraging the Hugging Face transformers library as the source of truth for model definitions; a wide diversity of models large and small is available to power text, audio, image and video AI applications. Eight million AI Builders use Hugging Face to collaborate on over two million AI models and datasets openly shared with the global community. We are excited to support the llm-d project to enable developers to take these applications to scale.”</p>
<p><em>Priya Nagpurkar, vice president, Hybrid Cloud and AI Platform, IBM Research</em><br>
<!-- -->“At IBM, we believe the next phase of AI is about efficiency and scale. We’re focused on unlocking value for enterprises through AI solutions they can deploy effectively. As a founding contributor to llm-d, IBM is proud to be a key part of building a differentiated hardware agnostic distributed AI inference platform. We’re looking forward to continued contributions towards the growth and success of this community to transform the future of AI inference.”</p>
<p><em>Bill Pearson, vice president, Data Center &amp; AI Software Solutions and Ecosystem, Intel</em><br>
<!-- -->“The launch of llm-d will serve as a key inflection point for the industry in driving AI transformation at scale, and Intel is excited to participate as a founding supporter.  Intel’s involvement with llm-d is the latest milestone in our decades-long collaboration with Red Hat to empower enterprises with open source solutions that they can deploy anywhere, on their platform of choice. We look forward to further extending and building AI innovation through the llm-d community.”</p>
<p><em>Eve Callicoat, senior staff engineer, ML Platform, Lambda</em><br>
<!-- -->&quot;Inference is where the real-world value of AI is delivered, and llm-d represents a major leap forward. Lambda is proud to support a project that makes state-of-the-art inference accessible, efficient, and open.&quot;</p>
<p><em>Ujval Kapasi, vice president, Engineering AI Frameworks, NVIDIA</em><br>
<!-- -->“The llm-d project is an important addition to the open source AI ecosystem and reflects NVIDIA’s support for collaboration to drive innovation in generative AI. Scalable, highly performant inference is key to the next wave of generative and agentic AI. We’re working with Red Hat and other supporting partners to foster llm-d community engagement and industry adoption, helping accelerate llm-d with innovations from NVIDIA Dynamo such as NIXL.”</p>
<p><em>Ion Stoica, Professor and Director of Sky Computing Lab, University of California, Berkeley</em><br>
<!-- -->“We are pleased to see Red Hat build upon the established success of vLLM, which originated in our lab to help address the speed and memory challenges that come with running large AI models. Open source projects like vLLM, and now llm-d anchored in vLLM, are at the frontier of AI innovation tackling the most demanding AI inference requirements and moving the needle for the industry at large.”</p>
<p><em>Junchen Jiang, CS Professor, LMCache Lab, University of Chicago</em><br>
<!-- -->“Distributed KV cache optimizations, such as offloading, compression, and blending, have been a key focus of our lab, and we are excited to see llm-d leveraging LMCache as a core component to reduce time to first token as well as improve throughput, particularly in long-context inference.”</p>
<p><strong>Additional Resources</strong></p>
<ul>
<li>Learn more about <a href="https://www.llm-d.ai" target="_blank" rel="noopener noreferrer">llm-d</a></li>
<li>Read more about <a href="https://www.redhat.com/en/topics/ai/what-is-vllm" target="_blank" rel="noopener noreferrer">vLLM</a></li>
<li>Find out more about <a href="https://github.com/llm-d" target="_blank" rel="noopener noreferrer">contributing to llm-d</a></li>
<li>Learn more about <a href="http://red.ht/I2Zk1e" target="_blank" rel="noopener noreferrer">Red Hat Summit</a></li>
<li>See all of Red Hat’s announcements this week in the <a href="https://red.ht/3QrRUAh" target="_blank" rel="noopener noreferrer">Red Hat Summit newsroom</a></li>
<li>Follow <a href="https://twitter.com/redhatsummit" target="_blank" rel="noopener noreferrer">@RedHatSummit</a> or <a href="https://twitter.com/hashtag/rhsummit" target="_blank" rel="noopener noreferrer">#RHSummit</a> on X for event-specific updates</li>
</ul>
<p><strong>Connect with Red Hat</strong></p>
<ul>
<li>Learn more about <a href="http://red.ht/IOS5vm" target="_blank" rel="noopener noreferrer">Red Hat</a></li>
<li>Get more news in the <a href="http://red.ht/1qeXuma" target="_blank" rel="noopener noreferrer">Red Hat newsroom</a></li>
<li>Read the <a href="http://red.ht/1zzgkXp" target="_blank" rel="noopener noreferrer">Red Hat blog</a></li>
<li>Follow <a href="https://red.ht/3Ghe0TT." target="_blank" rel="noopener noreferrer">Red Hat on X</a></li>
<li>Follow <a href="https://red.ht/4iBsqwB" target="_blank" rel="noopener noreferrer">Red Hat on Instagram</a></li>
<li>Follow <a href="https://red.ht/4hHewrv" target="_blank" rel="noopener noreferrer">Red Hat on LinkedIn</a></li>
<li>Watch <a href="https://red.ht/44B8oxL" target="_blank" rel="noopener noreferrer">Red Hat videos on YouTube</a></li>
</ul>
<p><strong>About Red Hat</strong><br>
<a href="https://www.redhat.com/en" target="_blank" rel="noopener noreferrer">Red Hat</a> is the open hybrid cloud technology leader, delivering a trusted, consistent and comprehensive foundation for transformative IT innovation and AI applications. Its portfolio of cloud, developer, AI, Linux, automation and application platform technologies enables any application, anywhere—from the datacenter to the edge. As the world&#x27;s leading provider of enterprise open source software solutions, Red Hat invests in open ecosystems and communities to solve tomorrow&#x27;s IT challenges. Collaborating with partners and customers, Red Hat helps them build, connect, automate, secure and manage their IT environments, supported by consulting services and <a href="https://access.redhat.com/recognition" target="_blank" rel="noopener noreferrer">award-winning</a> training and certification offerings.</p>
<p><strong>Forward-Looking Statements</strong><br>
<!-- -->Except for the historical information and discussions contained herein, statements contained in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are based on the company’s current assumptions regarding future business and financial performance. These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially. Any forward-looking statement in this press release speaks only as of the date on which it is made. Except as required by law, the company assumes no obligation to update or revise any forward-looking statements.</p>
<p><strong>Media Contact:</strong><br>
<!-- -->John Terrill<br>
<!-- -->Red Hat<br>
<!-- -->+1-571-421-8132<br>
<a href="mailto:jterrill@redhat.com" target="_blank" rel="noopener noreferrer">jterrill@redhat.com</a></p>
<p><em>###</em></p>
<p><em>Red Hat and the Red Hat logo are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the U.S. and other countries.</em></p>
<!-- -->
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_LWe7 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-dbc4ec">
<p>Forecast Analysis: AI Semiconductors, Worldwide, Alan Priestley, Gartner, 2 August 2024 - ID G00818912 GARTNER is a registered trademark and service mark of Gartner, Inc. and/or its affiliates in <a href="#user-content-fnref-1-dbc4ec" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Used for &quot;official&quot; news releases in the blog" class="tag_zVej tagRegular_sFm0" href="/llm-d.github.io/blog/tags/news-releases">News Releases</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/llm-d.github.io/blog/llm-d-announce"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">Announcing the llm-d community!</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#may-20-2025" class="table-of-contents__link toc-highlight">May 20, 2025</a><ul><li><a href="#answering-the-need-for-scalable-gen-ai-inference-with-llm-d" class="table-of-contents__link toc-highlight">Answering the need for scalable gen AI inference with llm-d</a></li><li><a href="#llm-d-backed-by-industry-leaders" class="table-of-contents__link toc-highlight">llm-d: Backed by industry leaders</a></li><li><a href="#red-hats-vision-any-model-any-accelerator-any-cloud" class="table-of-contents__link toc-highlight">Red Hat’s vision: Any model, any accelerator, any cloud.</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">User Guide</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/guide">How to Use</a></li></ul></div><div class="col footer__col"><div class="footer__title">Architecture</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/architecture/architecture">Overview</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/community/contact_us">Contact us</a></li><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/docs/community/contribute">Contributing</a></li><li class="footer__item"><a href="https://github.com/llm-d/llm-d/blob/dev/CODE_OF_CONDUCT.md" target="_blank" rel="noopener noreferrer" class="footer__link-item">Code of Conduct<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d.github.io/blog">News</a></li><li class="footer__item">
                <a href="https://github.com/llm-d/" target="_blank" rel="noreferrer noopener" aria-label="GitHub Icon">
                  <img class="github-footer" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/github-mark-white.png" alt="GitHub Icon" width="30px" height="auto" margin="5px">
                </a>
              </li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item">
                <a href="https://linkedin.com/company/llm-d" target="_blank" rel="noreferrer noopener" aria-label="LinkedIn Icon">
                  <img class="linkedin" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/linkedin-mark-white.png" alt="LinkedIn Icon" width="30px" height="auto" margin="5px">

                </a>
              </li><li class="footer__item">
                <a href="https://llm-d.slack.com" target="_blank" rel="noreferrer noopener" aria-label="Slack Icon">
                  <img class="slack" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/slack-mark-white.png" alt="Slack Icon" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://inviter.co/llm-d-slack" target="_blank" rel="noreferrer noopener" aria-label="Inviter Icon">
                  <span class="button-link">Join our Slack</span>    
                  <img class="inviter" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/inviter-logo.png" alt="Slack Inviter link" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://www.reddit.com/r/llm_d/" target="_blank" rel="noreferrer noopener" aria-label="Reddit Icon">
                  <img class="reddit" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/reddit-mark-white.png" alt="Reddit Icon" width="30px" height="auto" margin="5px">
                </a>
              </li><li class="footer__item">
                <a href="https://x.com/_llm_d_" target="_blank" rel="noreferrer noopener" aria-label="X Icon">
                  <img class="x" src="https://raw.githubusercontent.com/KPRoche/iconography/refs/heads/main/assets/x-mark-white.png" alt="X Icon" width="30px" height="auto" margin="5px">
                </a>
              </li></ul></div></div></div></footer></div>
</body>
</html>